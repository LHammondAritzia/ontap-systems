---
sidebar: upgrade-arl-manual-app_sidebar
permalink: upgrade-arl-manual-app/relocate_non_root_aggr_node1_node2.html
keywords: non-root aggregate, nonroot, move, relocate, nas
summary: Before you can replace node1 with node3, you must move the non-root aggregates from node1 to node2 by using the `storage aggregate relocation` command and then verifying the relocation.
---

= Relocate non-root aggregates owned by node1 to node2
:hardbreaks:
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ./media/

[.lead]
Before you can replace node1 with node3, you must move the non-root aggregates from node1 to node2 by using the `storage aggregate relocation` command and then verifying the relocation.

.Steps

. Relocate the non-root aggregates by completing the following substeps:

.. Set the privilege level to advanced:
+
`set -privilege advanced`

.. Enter the following command:

`storage aggregate relocation start -node <node1> -destination <node2> -aggregate-list * -ndo-controller-upgrade true`

.. When prompted, enter `y`.

Relocation will occur in the background. It could take anywhere from a few seconds to a couple of minutes to relocate an aggregate. The time includes both client outage and nonoutage portions. The command does not relocate any offline or restricted aggregates.

.. Return to the admin level by entering the following command:
+
`set -privilege admin`

. Check the relocation status by entering the following command on node1:
+
`storage aggregate relocation show -node node1`
+
The output will display Done for an aggregate after it has been relocated.
+
NOTE: Wait until all non-root aggregates owned by node1 have been relocated to node2 before proceeding to the next step.

. Take one of the following actions:
+
[cols="35,65"]
|===
|If relocation... |Then..

|Of all aggregates is successful
|Go to Step 4.
|Of any aggregates fails or is vetoed
a|
.. Check the EMS logs for the corrective action.

.. Perform the corrective action.

.. Relocate any failed or vetoed aggregates by entering the following command:
+
`storage aggregate relocation start -node node1 -destination node2 -aggregate-list * -ndo-controllerupgrade true`

.. When prompted, enter 'y'.

.. Return to the admin level by entering the following command:
+
`set -privilege admin`

If necessary, you can force the relocation using one of the following methods:

* Overriding veto checks by using the command `storage aggregate relocation start -override-vetoes true -ndo-controllerupgrade`

* Overriding destination checks by using the command `storage aggregate relocation start -override-destination-checks true -ndocontroller-upgrade`

See the _ONTAP 9 Disks and Aggregates Power Guide- and the _ONTAP 9 Commands: Manual Page Reference_ for more information about storage aggregate relocation commands.
|===

. Verify that all the non-root aggregates are online and their state on node2 by entering the following command:
+
`storage aggregate show -node node2 -state online -root false`
+
The following example shows that the non-root aggregates on node2 are online:
+
----
cluster::> storage aggregate show -node node2 state online -root false
Aggregate Size     Available Used% State   #Vols  Nodes            RAID Status
--------- -------- --------- ----- ------- ------ ---------------- ------------
aggr_1    744.9GB  744.8GB   0%    online   5     node2            raid_dp,
                                                                   normal
aggr_2    825.0GB  825.0GB   0%    online   1     node2            raid_dp,
                                                                   normal
2 entries were displayed.
----
+
If the aggregates have gone offline or become foreign on node2, bring them online by using the following command on node2, once for each aggregate:
+
`storage aggregate online -aggregate aggr_name`

. Verify that all the volumes are online on node2 by entering the following command on node2
and examining its output:
volume show -node node2 -state offline
If any volumes are offline on node2, bring them online by using the following command on
node2, once for each volume:
volume online -vserver vserver-name -volume volume-name
The vserver-name to use with this command is found in the output of the previous volume
show command.
6. Enter the following command on node2:
Using Aggregate Relocation to Manually Upgrade Controller Hardware Running ONTAP 9.8 and Later 29
Using aggregate relocation to upgrade controller hardware on a pair of nodes running ONTAP 9.8 or later
Draft • Highly Confidential - ProdOps Restricted • Processed: Thursday January 07 2021 12:36:17
storage failover show -node node2
The output should display the following message:
Node owns partner's aggregates as part of the nondisruptive controller upgrade
procedure.
.
7. Verify that node1 does not own any non-root aggregates that are online by entering the
following command:
storage aggregate show -owner-name node1 -ha-policy sfo -state online
The output should not display any online non-root aggregates, which have already been
relocated to node2.
Moving NAS data LIFs owned by node1 to node2
Before you can replace node1 with node3, you need t
//
. Resume the aggregate relocation and NAS data LIF move operations:
+
`system controller replace resume`
+
All the non-root aggregates and NAS data LIFs are migrated from node1 to node2.
+
The operation pauses to allow you to verify whether all node1 non-root aggregates and non-SAN data LIFs have been migrated to node2.

. Check the status of the aggregate relocation and NAS data LIF move operations:
+
`system controller replace show-details`

. With the operation still paused, verify that all the non-root aggregates are online and their state on node2:
+
`storage aggregate show -node node2 -state online -root false`
+
The following example shows that the non-root aggregates on node2 are online:
+
----
 cluster::> storage aggregate show -node node2 state online -root false

 Aggregate Size     Available Used% State   #Vols  Nodes       RAID Status
 --------- -------- --------- ----- ------- ------ ----------- ------------
 aggr_1
           744.9GB  744.8GB   0%    online   5     node2       raid_dp,
                                                               normal
 aggr_2    825.0GB  825.0GB   0%    online   1     node2       raid_dp,
                                                               normal
 2 entries were displayed.
----
+
If the aggregates have gone offline or become foreign on node2, bring them online by using the following command on node2, once for each aggregate:
+
`storage aggregate online -aggregate <aggr_name>`

. Verify that all the volumes are online on node2 by entering the following command on node2 and examining its output:
+
`volume show -node node2 -state offline`
+
If any volumes are offline on node2, bring them online by using the following command on node2, once for each volume:
+
`volume online -vserver <Vserver-name> -volume <volume-name>`
+
The `Vserver-name` to use with this command is found in the output of the previous `volume show` command.

. If any LIFs are down, set the administrative status of the LIFs to "up" by entering the following command, once for each LIF:
+
`network interface modify -vserver <Vserver_name> -lif <LIF_name> -home-node <node-name> -status-admin up`
